{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "Основная идея данной парадигмы вычисления заключается в разделении обработки данных на 2 этапа: map и reduce. На первом этапе каждая запись обарабатывается независимо. На втором этапе записи сортируются по выбранному ключу и все записи, принадлежащие одному ключу, обрабатываются в рамках одного процесса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера будем использовать подход hadoop streaming, при котором в качестве map и reduce могут выступать любые исполняемые файлы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Посчитаем среднюю корректность ответов для content_id\n",
    "\n",
    "\n",
    "датасет \"Kaggle Riiid Answers Correctness Prediction\"\n",
    "* row_id: (int64) идентификатор записи\n",
    "* timestamp: (int64) время от начала взаимодействия пользователя до завершения задания\n",
    "* user_id: (int32) идентификатор пользователя\n",
    "* content_id: (int16) идентификатор типа взаимодействия\n",
    "* content_type_id: (int8) 0 - вопрос, 1 - просмотр лекции\n",
    "* task_container_id: (int16) ID набора вопросов\n",
    "* user_answer: (int8) ответ пользователя. -1 для лекция\n",
    "* answered_correctly: (int8) маркер правильного овтета. -1 для лекций\n",
    "* prior_question_elapsed_time: (float32) время, затраченное пользователей на ответ \n",
    "после прослушивания лекции\n",
    "* prior_question_had_explanation: (bool) пользователю показали правильный ответ, \n",
    "после ответа на вопросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данильченко Вадим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    row = line.split(',')\n",
    "    print(\"{}\\t{}\".format(row[3], row[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "current_content_id = None\n",
    "answered_correctly_count = 0\n",
    "answered_correctly_sum = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    content_id, answered_correctly = line.split('\\t', 1)\n",
    "    try:\n",
    "        content_id = int(content_id)\n",
    "        answered_correctly = int(answered_correctly)\n",
    "        if answered_correctly==-1:\n",
    "            continue\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if content_id == current_content_id:\n",
    "        answered_correctly_sum += answered_correctly\n",
    "        answered_correctly_count += 1\n",
    "    else:\n",
    "        if current_content_id is not None:\n",
    "            print(\"{}\\t{}\".format(content_id, answered_correctly_sum / answered_correctly_count))\n",
    "        current_content_id = content_id\n",
    "        answered_correctly_count = 1\n",
    "        answered_correctly_sum = answered_correctly\n",
    "\n",
    "if current_content_id == content_id and answered_correctly_count != 0:\n",
    "    print(\"{}\\t{}\".format(content_id, answered_correctly_sum / answered_correctly_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "пример shell строки, которая производит аналогичные hadoop MapReduce операции.\n",
    "\n",
    "Для тестирования будем использовать сокращенную выборку из 1000 строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1000 ~/data/train_10m.csv | ./mapper.py | sort | ./reducer.py > result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\t1.0\r\n",
      "10046\t1.0\r\n",
      "10047\t0.0\r\n",
      "10048\t1.0\r\n",
      "10049\t1.0\r\n",
      "10070\t0.0\r\n",
      "10071\t0.0\r\n",
      "10072\t1.0\r\n",
      "10073\t1.0\r\n",
      "10090\t1.0\r\n"
     ]
    }
   ],
   "source": [
    "!head result.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mapred_output_63'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_dir = \"mapred_output_{}\".format(random.randint(0, 100))\n",
    "random_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `mapred_output_63': No such file or directory\n",
      "packageJobJar: [/tmp/hadoop-unjar7596701997786816959/] [] /tmp/streamjob2581672004335687599.jar tmpDir=null\n",
      "21/08/10 17:59:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/08/10 17:59:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/08/10 17:59:56 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "21/08/10 17:59:56 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "21/08/10 17:59:56 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\n",
      "21/08/10 17:59:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1628598864856_0007\n",
      "21/08/10 17:59:57 INFO conf.Configuration: resource-types.xml not found\n",
      "21/08/10 17:59:57 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/08/10 17:59:57 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n",
      "21/08/10 17:59:57 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n",
      "21/08/10 17:59:57 INFO impl.YarnClientImpl: Submitted application application_1628598864856_0007\n",
      "21/08/10 17:59:57 INFO mapreduce.Job: The url to track the job: http://172.17.0.2:8088/proxy/application_1628598864856_0007/\n",
      "21/08/10 17:59:57 INFO mapreduce.Job: Running job: job_1628598864856_0007\n",
      "21/08/10 18:00:02 INFO mapreduce.Job: Job job_1628598864856_0007 running in uber mode : false\n",
      "21/08/10 18:00:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/08/10 18:00:09 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "21/08/10 18:00:15 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/08/10 18:00:25 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "21/08/10 18:00:29 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/08/10 18:00:30 INFO mapreduce.Job: Job job_1628598864856_0007 completed successfully\n",
      "21/08/10 18:00:30 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=89578092\n",
      "\t\tFILE: Number of bytes written=180409181\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=557113146\n",
      "\t\tHDFS: Number of bytes written=311868\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=47065\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17744\n",
      "\t\tTotal time spent by all map tasks (ms)=47065\n",
      "\t\tTotal time spent by all reduce tasks (ms)=17744\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=47065\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=17744\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=48194560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18169856\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000000\n",
      "\t\tMap output records=10000000\n",
      "\t\tMap output bytes=69578086\n",
      "\t\tMap output materialized bytes=89578116\n",
      "\t\tInput split bytes=495\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=13750\n",
      "\t\tReduce shuffle bytes=89578116\n",
      "\t\tReduce input records=10000000\n",
      "\t\tReduce output records=13499\n",
      "\t\tSpilled Records=20000000\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=792\n",
      "\t\tCPU time spent (ms)=50500\n",
      "\t\tPhysical memory (bytes) snapshot=1859379200\n",
      "\t\tVirtual memory (bytes) snapshot=11852464128\n",
      "\t\tTotal committed heap usage (bytes)=1190133760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=557112651\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=311868\n",
      "21/08/10 18:00:30 INFO streaming.StreamJob: Output directory: mapred_output_63\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r $random_dir\n",
    "!hadoop jar /usr/local/hadoop-2.10.0/share/hadoop/tools/lib/hadoop-streaming.jar \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input train_10m.csv  \\\n",
    "-output $random_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0.9088277858176556\n",
      "10\t0.8939393939393939\n",
      "100\t0.6849534809111325\n",
      "1000\t0.35821872953503603\n",
      "10000\t0.7222946544980443\n",
      "10001\t0.540785498489426\n",
      "10002\t0.43529411764705883\n",
      "10003\t0.5970664365832614\n",
      "10004\t0.4175152749490835\n",
      "10009\t0.4227467811158798\n",
      "1001\t0.9297872340425531\n",
      "10010\t0.6476306196840826\n",
      "10011\t0.7851063829787234\n",
      "10012\t0.9319148936170213\n",
      "10013\t0.8702127659574468\n",
      "10014\t0.8954954954954955\n",
      "10015\t0.7351351351351352\n",
      "10016\t0.9261261261261261\n",
      "10017\t0.6324324324324324\n",
      "10018\t0.3902439024390244\n",
      "cat: Unable to write to output stream.\n",
      "Deleted mapred_output_63\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat $random_dir/* | head -n 20\n",
    "!hdfs dfs -rm -r $random_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
